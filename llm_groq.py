"""
llm_groq.py - Integraci√≥n con Groq API (Alternativa GRATUITA a Gemini)
‚úÖ 14,400 requests/d√≠a GRATIS
‚úÖ Streaming REAL (respuesta instant√°nea)
‚úÖ Modelos r√°pidos: Llama 3.1, Mixtral
"""
import asyncio
import re
from typing import AsyncGenerator
from groq import AsyncGroq
from config import Config
import unicodedata


class GroqLLM:
    """Clase para manejo del LLM con Groq API"""
    
    def __init__(self):
        self.client = None
        self.system_instruction = Config.SYSTEM_INSTRUCTION
        self.conversation_history = []
        
    def initialize(self):
        """Inicializa la API de Groq"""
        print("üß† Inicializando Groq API...")
        
        try:
            if not Config.GROQ_API_KEY:
                raise ValueError("GROQ_API_KEY no configurada")
            
            # Configurar cliente Groq
            self.client = AsyncGroq(api_key=Config.GROQ_API_KEY)
            self.conversation_history = []

            print("‚úÖ Groq API inicializado correctamente")
            print("üí° L√≠mite: 14,400 requests/d√≠a GRATIS")
            return True
            
        except Exception as e:
            print(f"‚ùå Error al inicializar Groq: {e}")
            print("üí° Verifica tu GROQ_API_KEY en el archivo .env")
            print("   Obt√©n una clave GRATIS en: https://console.groq.com/keys")
            return False

    def _build_messages(self, user_message: str) -> list:
        """Construye el array de mensajes para Groq"""
        messages = [
            {"role": "system", "content": self.system_instruction}
        ]
        
        # A√±adir historial
        for turn in self.conversation_history:
            messages.append({"role": "user", "content": turn["user"]})
            messages.append({"role": "assistant", "content": turn["assistant"]})
        
        # A√±adir mensaje actual
        messages.append({"role": "user", "content": user_message})
        
        return messages

    async def send_message_stream(
        self, 
        message: str
    ) -> AsyncGenerator[str, None]:
        """
        Env√≠a un mensaje y devuelve la respuesta en streaming REAL
        
        Args:
            message: Mensaje del usuario
            
        Yields:
            Fragmentos de texto de la respuesta EN TIEMPO REAL
        """
        try:
            print(f"üí¨ Enviando a Groq: {message}")
            
            messages = self._build_messages(message)
            
            # Hacer streaming con Groq
            stream = await self.client.chat.completions.create(
                model="llama-3.1-70b-versatile",  # Modelo r√°pido y gratuito
                messages=messages,
                stream=True,
                max_tokens=500,  # Respuestas cortas para conversaci√≥n
                temperature=0.9,
            )
            
            full_response = ""
            
            # Recibir chunks EN TIEMPO REAL
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    text = chunk.choices[0].delta.content
                    full_response += text
                    
                    # Normalizar para TTS
                    normalized = self.normalize_for_tts(text)
                    if normalized.strip():
                        yield normalized
            
            # Guardar en historial
            self.conversation_history.append({
                "user": message,
                "assistant": full_response
            })
            
            print(f"‚úÖ Respuesta completa recibida ({len(full_response)} chars)")
            
        except Exception as e:
            print(f"‚ùå Error en Groq: {e}")
            yield "[ERROR] Lo siento, tuve un problema procesando tu mensaje."
    
    async def get_response_with_chunking(
        self, 
        message: str,
        chunk_callback=None
    ) -> str:
        """
        Obtiene respuesta con chunking inteligente (por puntuaci√≥n)
        
        Args:
            message: Mensaje del usuario
            chunk_callback: Funci√≥n as√≠ncrona a llamar con cada chunk completo
            
        Returns:
            Respuesta completa
        """
        buffer = ""
        full_response = ""
        
        async for text_chunk in self.send_message_stream(message):
            buffer += text_chunk
            full_response += text_chunk
            
            # Buscar puntos naturales de corte
            chunks = self._split_on_punctuation(buffer)
            
            # Si hay chunks completos, enviarlos
            if len(chunks) > 1:
                for chunk in chunks[:-1]:
                    if chunk.strip() and chunk_callback:
                        await chunk_callback(chunk.strip())
                
                # Mantener el √∫ltimo fragmento incompleto en el buffer
                buffer = chunks[-1]
        
        # Enviar el √∫ltimo chunk si queda algo
        if buffer.strip() and chunk_callback:
            await chunk_callback(buffer.strip())
        
        return full_response
    
    def _split_on_punctuation(self, text: str) -> list[str]:
        """Divide el texto en puntos naturales de corte"""
        pattern = r'([.!?;,])\s+'
        parts = re.split(pattern, text)
        
        chunks = []
        i = 0
        while i < len(parts):
            if i + 1 < len(parts) and parts[i + 1] in '.!?;,':
                chunks.append(parts[i] + parts[i + 1])
                i += 2
            else:
                chunks.append(parts[i])
                i += 1
        
        return chunks
    
    def extract_emotion(self, text: str) -> tuple[str, str]:
        """Extrae la etiqueta de emoci√≥n del texto"""
        match = re.match(r'\[(.*?)\]\s*(.*)', text, re.IGNORECASE)
        
        if match:
            emotion = match.group(1).upper()
            clean_text = match.group(2)
            return emotion, clean_text
        
        return "NEUTRAL", text
    
    def add_time_pressure(self, seconds_remaining: int):
        """A√±ade presi√≥n de tiempo al contexto"""
        if seconds_remaining <= 30:
            time_message = f"[SISTEMA] Quedan {seconds_remaining} segundos. Desp√≠dete naturalmente."
            self.conversation_history.append({
                "user": "",
                "assistant": time_message
            })
            print(f"‚è∞ Presi√≥n de tiempo a√±adida: {seconds_remaining}s restantes")

    def normalize_for_tts(self, text: str) -> str:
        """Normaliza texto para TTS (sin acentos que Piper pronuncia mal)"""
        # Normalizar unicode
        text = unicodedata.normalize("NFD", text)
        
        # Quitar acentos pero conservar √±/√ë
        text = "".join(
            c for c in text
            if unicodedata.category(c) != "Mn" or c.lower() == "n"
        )
        
        # Solo letras, √±, n√∫meros, espacio, coma y punto
        text = re.sub(r"[^a-zA-Z√±√ë0-9\s.,!?]", "", text)
        
        # Normalizar espacios
        text = re.sub(r"\s+", " ", text).strip()
        
        return text

    def reset_conversation(self):
        """Reinicia la conversaci√≥n"""
        self.conversation_history = []
        print("üîÑ Conversaci√≥n reiniciada")


# Ejemplo de uso
async def main():
    llm = GroqLLM()
    
    if not llm.initialize():
        return
    
    try:
        async def on_chunk(chunk: str):
            emotion, text = llm.extract_emotion(chunk)
            print(f"üé≠ [{emotion}] {text}")
        
        test_messages = [
            "Hola, ¬øc√≥mo est√°s?",
            "Cu√©ntame un chiste corto",
            "¬°Eres muy tonta!"
        ]
        
        for msg in test_messages:
            print(f"\nüë§ Usuario: {msg}")
            await llm.get_response_with_chunking(msg, chunk_callback=on_chunk)
            await asyncio.sleep(1)
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Interrumpido por usuario")


if __name__ == "__main__":
    asyncio.run(main())
